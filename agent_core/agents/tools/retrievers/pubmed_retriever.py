# agent_core/agents/tools/retrievers/pubmed_retriever.py
# åŸºäºä½ çš„å·¥ä½œä»£ç æ”¹è¿›çš„PubMedæ£€ç´¢å™¨

import asyncio
import concurrent.futures
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime
from dataclasses import dataclass, asdict
from Bio import Entrez

logger = logging.getLogger(__name__)

# è®¾ç½®Entrezå‚æ•°
Entrez.email = "czqrainy@gmail.com"
Entrez.api_key = "983222f9d5a2a81facd7d158791d933e6408"

@dataclass
class PubMedArticle:
    """PubMedæ–‡ç« æ•°æ®ç»“æ„"""
    pmid: str
    title: str
    abstract: str
    authors: List[str]
    journal: str
    publication_date: str
    doi: Optional[str] = None
    pmc_id: Optional[str] = None
    keywords: Optional[List[str]] = None
    mesh_terms: Optional[List[str]] = None
    url: str = ""
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []
        if self.mesh_terms is None:
            self.mesh_terms = []
        if not self.url:
            self.url = f"https://pubmed.ncbi.nlm.nih.gov/{self.pmid}/"

@dataclass
class PubMedSearchResult:
    """PubMedæœç´¢ç»“æœ"""
    query: str
    total_count: int
    retrieved_count: int
    articles: List[PubMedArticle]
    search_timestamp: str
    api_version: str = "2.0.0"

def get_pubmed_abstracts(query: str, retmax: int = 20) -> List[Dict]:
    """
    åŸå§‹å·¥ä½œå‡½æ•° - ä¿æŒä¸å˜ä»¥ç¡®ä¿å…¼å®¹æ€§
    è¿”å› Title / Abstract / PMID çš„åˆ—è¡¨
    """
    try:
        # â‘  esearchï¼šæ‹¿ PMID åˆ—è¡¨
        ids = Entrez.read(
            Entrez.esearch(db="pubmed", term=query, retmax=retmax)
        )["IdList"]
        
        if not ids:
            return []
        
        # â‘¡ efetchï¼šä¸€æ¬¡æ€§æŠŠæ‘˜è¦æŠ“ä¸‹æ¥ï¼ˆXMLï¼‰
        root = ET.fromstring(
            Entrez.efetch(db="pubmed", id=",".join(ids), retmode="xml").read()
        )
        
        records = []
        for art in root.findall(".//PubmedArticle"):
            pmid = art.findtext(".//PMID")
            title = art.findtext(".//ArticleTitle", default="No title")
            abstract = art.findtext(".//AbstractText", default="No abstract")
            records.append({"PMID": pmid, "Title": title, "Abstract": abstract})
        
        return records
        
    except Exception as e:
        logger.error(f"PubMedæ£€ç´¢å¤±è´¥: {e}")
        return []

def get_pubmed_articles_enhanced(query: str, retmax: int = 20) -> List[PubMedArticle]:
    """
    å¢å¼ºç‰ˆå‡½æ•° - æå–æ›´å¤šä¿¡æ¯
    åŸºäºä½ çš„å·¥ä½œä»£ç ï¼Œæ·»åŠ æ›´å¤šå­—æ®µæå–
    """
    try:
        # â‘  esearchï¼šæ‹¿ PMID åˆ—è¡¨
        search_result = Entrez.read(
            Entrez.esearch(db="pubmed", term=query, retmax=retmax)
        )
        
        ids = search_result["IdList"]
        
        if not ids:
            return []
        
        # â‘¡ efetchï¼šä¸€æ¬¡æ€§æŠŠæ‘˜è¦æŠ“ä¸‹æ¥ï¼ˆXMLï¼‰
        xml_content = Entrez.efetch(db="pubmed", id=",".join(ids), retmode="xml").read()
        root = ET.fromstring(xml_content)
        
        articles = []
        for art in root.findall(".//PubmedArticle"):
            article = _extract_enhanced_article_info(art)
            if article:
                articles.append(article)
        
        return articles
        
    except Exception as e:
        logger.error(f"PubMedå¢å¼ºæ£€ç´¢å¤±è´¥: {e}")
        return []

def _extract_enhanced_article_info(article_xml: ET.Element) -> Optional[PubMedArticle]:
    """ä»XMLä¸­æå–å¢å¼ºçš„æ–‡ç« ä¿¡æ¯"""
    try:
        # åŸºæœ¬ä¿¡æ¯
        pmid = article_xml.findtext(".//PMID")
        if not pmid:
            return None
        
        title = article_xml.findtext(".//ArticleTitle", default="No title")
        
        # æ‘˜è¦ - å¤„ç†å¤šä¸ªAbstractTextå…ƒç´ 
        abstract_elements = article_xml.findall(".//AbstractText")
        if abstract_elements:
            abstract_parts = []
            for elem in abstract_elements:
                text = elem.text or ""
                label = elem.get('Label', '')
                if label and label.upper() not in ['UNLABELLED']:
                    text = f"{label}: {text}"
                if text.strip():
                    abstract_parts.append(text.strip())
            abstract = " ".join(abstract_parts)
        else:
            abstract = "No abstract available"
        
        # ä½œè€…
        authors = []
        for author in article_xml.findall(".//Author"):
            last_name = author.findtext("LastName", "")
            first_name = author.findtext("ForeName", "")
            if last_name and first_name:
                authors.append(f"{last_name}, {first_name}")
            elif last_name:
                authors.append(last_name)
        
        # æœŸåˆŠä¿¡æ¯
        journal = article_xml.findtext(".//Journal/Title", "Unknown Journal")
        if not journal or journal == "Unknown Journal":
            journal = article_xml.findtext(".//Journal/ISOAbbreviation", "Unknown Journal")
        
        # å‘è¡¨æ—¥æœŸ
        pub_date = ""
        date_elem = article_xml.find(".//PubDate")
        if date_elem is not None:
            year = date_elem.findtext("Year", "")
            month = date_elem.findtext("Month", "")
            day = date_elem.findtext("Day", "")
            
            if year:
                pub_date = year
                if month:
                    pub_date += f"-{month.zfill(2) if month.isdigit() else month[:3]}"
                    if day:
                        pub_date += f"-{day.zfill(2)}"
        
        # DOI
        doi = None
        for article_id in article_xml.findall(".//ArticleId"):
            if article_id.get('IdType') == 'doi':
                doi = article_id.text
                break
        
        # PMC ID
        pmc_id = None
        for article_id in article_xml.findall(".//ArticleId"):
            if article_id.get('IdType') == 'pmc':
                pmc_id = article_id.text
                break
        
        # MeSHå…³é”®è¯
        mesh_terms = []
        for mesh in article_xml.findall(".//MeshHeading/DescriptorName"):
            if mesh.text:
                mesh_terms.append(mesh.text.strip())
        
        # å…³é”®è¯
        keywords = []
        for keyword in article_xml.findall(".//Keyword"):
            if keyword.text:
                keywords.append(keyword.text.strip())
        
        return PubMedArticle(
            pmid=pmid,
            title=title.strip() if title else "No title",
            abstract=abstract,
            authors=authors,
            journal=journal,
            publication_date=pub_date,
            doi=doi,
            pmc_id=pmc_id,
            keywords=keywords,
            mesh_terms=mesh_terms
        )
        
    except Exception as e:
        logger.error(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
        return None

class PubMedRetriever:
    """
    å¼‚æ­¥PubMedæ£€ç´¢å™¨ - åŸºäºä½ çš„å·¥ä½œä»£ç 
    åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡ŒåŒæ­¥çš„Bio.Entrezä»£ç 
    """
    
    def __init__(self, 
                 email: str = "czqrainy@gmail.com",
                 api_key: Optional[str] = "983222f9d5a2a81facd7d158791d933e6408"):
        self.email = email
        self.api_key = api_key
        
        # è®¾ç½®Entrezå‚æ•°
        Entrez.email = email
        if api_key:
            Entrez.api_key = api_key
        
        logger.info(f"åˆå§‹åŒ–PubMedæ£€ç´¢å™¨ - åŸºäºå·¥ä½œä»£ç ")
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass
    
    async def search_literature(self, 
                              query: str, 
                              max_results: int = 20,
                              enhanced: bool = True) -> PubMedSearchResult:
        """
        å¼‚æ­¥æœç´¢æ–‡çŒ®
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            enhanced: æ˜¯å¦ä½¿ç”¨å¢å¼ºä¿¡æ¯æå–
        
        Returns:
            PubMedSearchResultå¯¹è±¡
        """
        search_timestamp = datetime.now().isoformat()
        
        try:
            logger.info(f"æœç´¢PubMedæ–‡çŒ®: {query} (æœ€å¤š {max_results} ç¯‡)")
            
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ä»£ç 
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as pool:
                if enhanced:
                    articles = await loop.run_in_executor(
                        pool, get_pubmed_articles_enhanced, query, max_results
                    )
                else:
                    # ä½¿ç”¨åŸå§‹å‡½æ•°å¹¶è½¬æ¢æ ¼å¼
                    raw_results = await loop.run_in_executor(
                        pool, get_pubmed_abstracts, query, max_results
                    )
                    articles = [
                        PubMedArticle(
                            pmid=result["PMID"],
                            title=result["Title"],
                            abstract=result["Abstract"],
                            authors=[],
                            journal="Unknown",
                            publication_date="",
                            keywords=[],
                            mesh_terms=[]
                        )
                        for result in raw_results
                    ]
            
            logger.info(f"æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡çŒ®")
            
            return PubMedSearchResult(
                query=query,
                total_count=len(articles),  # å®é™…è·å–çš„æ•°é‡
                retrieved_count=len(articles),
                articles=articles,
                search_timestamp=search_timestamp
            )
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®æœç´¢å¤±è´¥: {e}")
            return PubMedSearchResult(
                query=query,
                total_count=0,
                retrieved_count=0,
                articles=[],
                search_timestamp=search_timestamp
            )
    
    async def search_by_gene(self, 
                           gene: str, 
                           additional_terms: Optional[List[str]] = None,
                           max_results: int = 20) -> PubMedSearchResult:
        """
        æŒ‰åŸºå› åç§°æœç´¢æ–‡çŒ®
        
        Args:
            gene: åŸºå› åç§°
            additional_terms: é¢å¤–æœç´¢è¯
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            PubMedSearchResultå¯¹è±¡
        """
        # æ„å»ºç®€å•ä½†æœ‰æ•ˆçš„æŸ¥è¯¢
        query_parts = [gene]
        
        if additional_terms:
            query_parts.extend(additional_terms)
        
        # ä½¿ç”¨ç®€å•çš„ANDè¿æ¥ï¼Œé¿å…å¤æ‚çš„å­—æ®µé™å®šç¬¦
        query = " AND ".join(query_parts)
        
        return await self.search_literature(query, max_results)


# æµ‹è¯•å‡½æ•°
async def test_working_pubmed_retriever():
    """æµ‹è¯•åŸºäºå·¥ä½œä»£ç çš„æ£€ç´¢å™¨"""
    print("ğŸ§ª æµ‹è¯•åŸºäºå·¥ä½œä»£ç çš„PubMedæ£€ç´¢å™¨")
    print("=" * 50)
    
    # é¦–å…ˆæµ‹è¯•åŸå§‹å‡½æ•°
    print("1. æµ‹è¯•åŸå§‹å·¥ä½œå‡½æ•°:")
    try:
        results = get_pubmed_abstracts("BRCA1", retmax=3)
        print(f"   âœ… åŸå§‹å‡½æ•°æˆåŠŸ: {len(results)} ç¯‡æ–‡çŒ®")
        
        for i, result in enumerate(results, 1):
            print(f"   æ–‡çŒ® {i}: [{result['PMID']}] {result['Title'][:60]}...")
    except Exception as e:
        print(f"   âŒ åŸå§‹å‡½æ•°å¤±è´¥: {e}")
    
    print("\n2. æµ‹è¯•å¢å¼ºç‰ˆå‡½æ•°:")
    try:
        enhanced_results = get_pubmed_articles_enhanced("BRCA1", retmax=3)
        print(f"   âœ… å¢å¼ºç‰ˆæˆåŠŸ: {len(enhanced_results)} ç¯‡æ–‡çŒ®")
        
        for i, article in enumerate(enhanced_results, 1):
            print(f"   æ–‡çŒ® {i}: [{article.pmid}] {article.title[:60]}...")
            print(f"           æœŸåˆŠ: {article.journal}")
            print(f"           ä½œè€…: {', '.join(article.authors[:2])}...")
    except Exception as e:
        print(f"   âŒ å¢å¼ºç‰ˆå¤±è´¥: {e}")
    
    print("\n3. æµ‹è¯•å¼‚æ­¥æ£€ç´¢å™¨:")
    async with PubMedRetriever() as retriever:
        try:
            result = await retriever.search_by_gene("TP53", ["cancer"], max_results=3)
            print(f"   âœ… å¼‚æ­¥æ£€ç´¢å™¨æˆåŠŸ: {result.retrieved_count} ç¯‡æ–‡çŒ®")
            
            for i, article in enumerate(result.articles, 1):
                print(f"   æ–‡çŒ® {i}: [{article.pmid}] {article.title[:60]}...")
                print(f"           æœŸåˆŠ: {article.journal}")
                print(f"           æ—¥æœŸ: {article.publication_date}")
        except Exception as e:
            print(f"   âŒ å¼‚æ­¥æ£€ç´¢å™¨å¤±è´¥: {e}")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_working_pubmed_retriever())