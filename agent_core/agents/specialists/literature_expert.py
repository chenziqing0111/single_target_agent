# agent_core/agents/specialists/literature_expert.py - åŸºäºRAGçš„æ–‡çŒ®åˆ†æä¸“å®¶

"""
ğŸ§¬ Literature Expert - æ–‡çŒ®åˆ†æä¸“å®¶
æ”¯æŒRAGä¼˜åŒ–çš„å¤§è§„æ¨¡æ–‡çŒ®åˆ†æï¼Œå¤§å¹…èŠ‚çœTokenæ¶ˆè€—
"""

import sys
import os
import asyncio
import hashlib
import pickle
import logging
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# æ ¸å¿ƒä¾èµ–
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import faiss
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œè¯·å®‰è£…: pip install sentence-transformers faiss-cpu")
    raise e

from Bio import Entrez
import xml.etree.ElementTree as ET

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from agent_core.clients.llm_client import call_llm
from agent_core.config.analysis_config import AnalysisConfig, AnalysisMode, ConfigManager

logger = logging.getLogger(__name__)

# ===== æŸ¥è¯¢ç±»å‹æšä¸¾ =====

from enum import Enum

class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹"""
    GENE = "gene"                    # åŸºå› æŸ¥è¯¢
    KEYWORD = "keyword"              # å…³é”®è¯æŸ¥è¯¢
    PROTEIN_FAMILY = "protein_family" # è›‹ç™½å®¶æ—æŸ¥è¯¢
    MECHANISM = "mechanism"          # æœºåˆ¶æŸ¥è¯¢
    COMPLEX = "complex"              # å¤åˆæŸ¥è¯¢

# ===== æ•°æ®ç»“æ„å®šä¹‰ =====

@dataclass
class SearchQuery:
    """æœç´¢æŸ¥è¯¢ç»“æ„"""
    query_text: str                 # æŸ¥è¯¢æ–‡æœ¬
    query_type: QueryType           # æŸ¥è¯¢ç±»å‹
    additional_terms: List[str] = None  # é™„åŠ æœ¯è¯­
    exclude_terms: List[str] = None     # æ’é™¤æœ¯è¯­
    date_range: tuple = None            # æ—¥æœŸèŒƒå›´ (start_year, end_year)
    # max_results: int = 500              # æœ€å¤§ç»“æœæ•°
    max_results: int = 10              # æœ€å¤§ç»“æœæ•°

    
    def __post_init__(self):
        if self.additional_terms is None:
            self.additional_terms = []
        if self.exclude_terms is None:
            self.exclude_terms = []

@dataclass
class LiteratureDocument:
    """æ–‡çŒ®æ–‡æ¡£ç»“æ„"""
    pmid: str
    title: str
    abstract: str
    authors: List[str] = None
    journal: str = ""
    year: int = 0
    doi: str = ""
    
    def __post_init__(self):
        if self.authors is None:
            self.authors = []
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºå¯æœç´¢çš„æ–‡æœ¬"""
        return f"æ ‡é¢˜: {self.title}\næ‘˜è¦: {self.abstract}"

@dataclass
class TextChunk:
    """æ–‡æœ¬å—ç»“æ„"""
    text: str
    doc_id: str  # PMID
    chunk_id: str
    metadata: Dict
    
    def __post_init__(self):
        if not self.chunk_id:
            self.chunk_id = hashlib.md5(f"{self.doc_id}_{self.text[:50]}".encode()).hexdigest()[:12]

@dataclass
class LiteratureAnalysisResult:
    """æ–‡çŒ®åˆ†æç»“æœ"""
    gene_target: str
    disease_mechanism: str
    treatment_strategy: str
    target_analysis: str
    references: List[Dict]
    total_literature: int
    total_chunks: int
    confidence_score: float
    analysis_method: str
    timestamp: str
    config_used: Dict
    token_usage: Dict

# ===== PubMedæ£€ç´¢å™¨ =====

class PubMedRetriever:
    """PubMedæ–‡çŒ®æ£€ç´¢å™¨ - æ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹"""
    
    def __init__(self):
        self.name = "Enhanced PubMed Retriever"
        self.version = "3.0.0"
        # é…ç½®Bio.Entrez
        Entrez.email = "czqrainy@gmail.com"
        Entrez.api_key = "983222f9d5a2a81facd7d158791d933e6408"
        
        # é¢„å®šä¹‰çš„æœç´¢æ¨¡æ¿
        self.search_templates = {
            QueryType.GENE: [
                "{query}[Title/Abstract]",
                '"{query}" AND (disease OR treatment OR therapy)',
                "{query} AND (clinical trial[Publication Type] OR clinical study[Publication Type])",
                "{query} AND (mechanism OR pathway OR function)",
                "{query} AND (drug OR inhibitor OR target OR therapeutic)"
            ],
            QueryType.KEYWORD: [
                "{query}[Title/Abstract]",
                '"{query}" AND (regulation OR expression OR function)',
                "{query} AND (signaling OR pathway OR mechanism)",
                "{query} AND (therapeutic OR treatment OR clinical)",
                "{query} AND (protein OR gene OR molecular)"
            ],
            QueryType.PROTEIN_FAMILY: [
                '"{query}" AND (protein OR family OR domain)',
                "{query} AND (structure OR function OR binding)",
                "{query} AND (regulation OR expression OR localization)",
                "{query} AND (interaction OR complex OR assembly)",
                "{query} AND (evolution OR conservation OR phylogeny)"
            ],
            QueryType.MECHANISM: [
                '"{query}" AND (mechanism OR pathway OR process)',
                "{query} AND (regulation OR control OR modulation)",
                "{query} AND (signaling OR cascade OR network)",
                "{query} AND (molecular OR cellular OR biological)",
                "{query} AND (function OR role OR activity)"
            ],
            QueryType.COMPLEX: [
                "{query}",  # å¤åˆæŸ¥è¯¢ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                '"{query}" AND review[Publication Type]',
                "{query} AND recent[Filter]"
            ]
        }
    
    async def search_literature(self, search_query: Union[str, SearchQuery], max_results: int = 10) -> List[LiteratureDocument]:
        """
        æ£€ç´¢æ–‡çŒ® - æ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹
        
        Args:
            search_query: æŸ¥è¯¢å­—ç¬¦ä¸²æˆ–SearchQueryå¯¹è±¡
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            æ–‡çŒ®æ–‡æ¡£åˆ—è¡¨
        """
        
        # å¤„ç†è¾“å…¥å‚æ•°
        if isinstance(search_query, str):
            # å…¼å®¹åŸæœ‰æ¥å£ï¼šå­—ç¬¦ä¸²æŸ¥è¯¢é»˜è®¤ä¸ºåŸºå› æŸ¥è¯¢
            query = SearchQuery(
                query_text=search_query,
                query_type=QueryType.GENE,
                max_results=max_results
            )
        else:
            query = search_query
            max_results = query.max_results
        
        print(f"ğŸ“š æ£€ç´¢æ–‡çŒ®: {query.query_text} ({query.query_type.value})")
        print(f"   ç›®æ ‡: {max_results} ç¯‡")
        
        # æ„å»ºæœç´¢ç­–ç•¥
        search_strategies = self._build_search_strategies(query)
        
        all_documents = []
        seen_pmids = set()
        
        for strategy in search_strategies:
            print(f"  ğŸ” æœç´¢ç­–ç•¥: {strategy}")
            
            try:
                docs = await self._execute_search(strategy, max_results // len(search_strategies))
                
                for doc in docs:
                    if doc.pmid not in seen_pmids:
                        seen_pmids.add(doc.pmid)
                        all_documents.append(doc)
                        
                        if len(all_documents) >= max_results:
                            break
                
                print(f"    âœ… æ–°å¢ {len(docs)} ç¯‡ï¼Œç´¯è®¡ {len(all_documents)} ç¯‡")
                
                if len(all_documents) >= max_results:
                    break
                    
            except Exception as e:
                print(f"    âŒ æœç´¢å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“Š æ£€ç´¢å®Œæˆ: å…± {len(all_documents)} ç¯‡æ–‡çŒ®")
        return all_documents[:max_results]
    
    def _build_search_strategies(self, query: SearchQuery) -> List[str]:
        """æ„å»ºæœç´¢ç­–ç•¥"""
        
        base_strategies = self.search_templates.get(query.query_type, self.search_templates[QueryType.KEYWORD])
        strategies = []
        
        # åŸºç¡€æŸ¥è¯¢ç­–ç•¥
        for template in base_strategies:
            strategy = template.format(query=query.query_text)
            strategies.append(strategy)
        
        # æ·»åŠ é™„åŠ æœ¯è¯­
        if query.additional_terms:
            additional_query = f"({query.query_text}) AND ({' OR '.join(query.additional_terms)})"
            strategies.append(additional_query)
        
        # å¤„ç†æ’é™¤æœ¯è¯­
        if query.exclude_terms:
            exclude_part = " AND ".join([f"NOT {term}" for term in query.exclude_terms])
            enhanced_strategies = []
            for strategy in strategies[:2]:  # åªå¯¹å‰ä¸¤ä¸ªç­–ç•¥åº”ç”¨æ’é™¤
                enhanced_strategies.append(f"{strategy} {exclude_part}")
            strategies.extend(enhanced_strategies)
        
        # æ—¥æœŸèŒƒå›´è¿‡æ»¤
        if query.date_range:
            start_year, end_year = query.date_range
            date_filter = f" AND {start_year}[PDAT]:{end_year}[PDAT]"
            dated_strategies = []
            for strategy in strategies[:3]:  # å¯¹å‰ä¸‰ä¸ªç­–ç•¥åº”ç”¨æ—¥æœŸè¿‡æ»¤
                dated_strategies.append(f"{strategy}{date_filter}")
            strategies.extend(dated_strategies)
        
        return strategies
    
    async def _execute_search(self, query: str, max_results: int) -> List[LiteratureDocument]:
        """æ‰§è¡Œå•æ¬¡æœç´¢"""
        
        try:
            # 1. æœç´¢PMID
            search_handle = Entrez.esearch(
                db="pubmed", 
                term=query, 
                retmax=max_results,
                sort="relevance"
            )
            search_results = Entrez.read(search_handle)
            pmid_list = search_results["IdList"]
            
            if not pmid_list:
                return []
            
            # 2. æ‰¹é‡è·å–è¯¦æƒ…
            documents = []
            batch_size = 50
            
            for i in range(0, len(pmid_list), batch_size):
                batch_pmids = pmid_list[i:i+batch_size]
                batch_docs = await self._fetch_batch_details(batch_pmids)
                documents.extend(batch_docs)
                
                # APIé™æµ
                if i + batch_size < len(pmid_list):
                    await asyncio.sleep(0.5)
            
            return documents
            
        except Exception as e:
            print(f"âŒ æœç´¢æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    async def _fetch_batch_details(self, pmid_list: List[str]) -> List[LiteratureDocument]:
        """æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…"""
        
        try:
            fetch_handle = Entrez.efetch(
                db="pubmed",
                id=",".join(pmid_list),
                rettype="medline",
                retmode="xml"
            )
            
            root = ET.fromstring(fetch_handle.read())
            
            documents = []
            for article in root.findall(".//PubmedArticle"):
                doc = self._parse_article(article)
                if doc and doc.abstract:  # åªä¿ç•™æœ‰æ‘˜è¦çš„
                    documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å–å¤±è´¥: {e}")
            return []
    
    def _parse_article(self, article_xml) -> Optional[LiteratureDocument]:
        """è§£æå•ç¯‡æ–‡ç« """
        
        try:
            # åŸºæœ¬ä¿¡æ¯
            pmid = article_xml.findtext(".//PMID", "")
            title = article_xml.findtext(".//ArticleTitle", "")
            
            # æ‘˜è¦å¤„ç†
            abstract_elem = article_xml.find(".//Abstract")
            abstract = ""
            if abstract_elem is not None:
                abstract_texts = []
                for text_elem in abstract_elem.findall(".//AbstractText"):
                    text = text_elem.text or ""
                    label = text_elem.get("Label", "")
                    if label:
                        abstract_texts.append(f"{label}: {text}")
                    else:
                        abstract_texts.append(text)
                abstract = " ".join(abstract_texts)
            
            # ä½œè€…
            authors = []
            for author in article_xml.findall(".//Author"):
                last_name = author.findtext("LastName", "")
                first_name = author.findtext("ForeName", "")
                if last_name:
                    authors.append(f"{first_name} {last_name}".strip())
            
            # æœŸåˆŠå’Œå¹´ä»½
            journal = article_xml.findtext(".//Journal/Title", "")
            year_elem = article_xml.find(".//PubDate/Year")
            year = int(year_elem.text) if year_elem is not None and year_elem.text else 0
            
            # DOI
            doi = ""
            for article_id in article_xml.findall(".//ArticleId"):
                if article_id.get("IdType") == "doi":
                    doi = article_id.text or ""
                    break
            
            if not title or not abstract:
                return None
            
            return LiteratureDocument(
                pmid=pmid,
                title=title,
                abstract=abstract,
                authors=authors,
                journal=journal,
                year=year,
                doi=doi
            )
            
        except Exception as e:
            return None

# ===== æ–‡æœ¬åˆ†å—å™¨ =====

class SmartChunker:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 250, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_documents(self, documents: List[LiteratureDocument]) -> List[TextChunk]:
        """åˆ†å—æ–‡æ¡£"""
        
        print(f"ğŸ“ å¼€å§‹æ–‡æœ¬åˆ†å—ï¼Œå—å¤§å°: {self.chunk_size}")
        
        all_chunks = []
        for doc in documents:
            chunks = self._chunk_single_doc(doc)
            all_chunks.extend(chunks)
        
        print(f"âœ… åˆ†å—å®Œæˆ: {len(documents)} ç¯‡ â†’ {len(all_chunks)} å—")
        return all_chunks
    
    def _chunk_single_doc(self, doc: LiteratureDocument) -> List[TextChunk]:
        """åˆ†å—å•ä¸ªæ–‡æ¡£"""
        
        chunks = []
        
        # 1. æ ‡é¢˜å—ï¼ˆé‡è¦ï¼‰
        title_chunk = TextChunk(
            text=f"æ ‡é¢˜: {doc.title}",
            doc_id=doc.pmid,
            chunk_id=f"{doc.pmid}_title",
            metadata={
                "pmid": doc.pmid,
                "title": doc.title,
                "journal": doc.journal,
                "year": doc.year,
                "chunk_type": "title"
            }
        )
        chunks.append(title_chunk)
        
        # 2. æ‘˜è¦åˆ†å—
        abstract_chunks = self._chunk_abstract(doc)
        chunks.extend(abstract_chunks)
        
        return chunks
    
    def _chunk_abstract(self, doc: LiteratureDocument) -> List[TextChunk]:
        """åˆ†å—æ‘˜è¦"""
        
        abstract = doc.abstract
        if len(abstract) <= self.chunk_size:
            # çŸ­æ‘˜è¦ï¼Œæ•´ä½“ä½œä¸ºä¸€å—
            return [TextChunk(
                text=f"æ‘˜è¦: {abstract}",
                doc_id=doc.pmid,
                chunk_id=f"{doc.pmid}_abstract",
                metadata={
                    "pmid": doc.pmid,
                    "title": doc.title,
                    "journal": doc.journal,
                    "year": doc.year,
                    "chunk_type": "abstract"
                }
            )]
        
        # é•¿æ‘˜è¦ï¼ŒæŒ‰å¥å­åˆ†å—
        sentences = self._split_sentences(abstract)
        chunks = []
        current_chunk = ""
        chunk_index = 0
        
        for sentence in sentences:
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(TextChunk(
                        text=f"æ‘˜è¦: {current_chunk}",
                        doc_id=doc.pmid,
                        chunk_id=f"{doc.pmid}_abstract_{chunk_index}",
                        metadata={
                            "pmid": doc.pmid,
                            "title": doc.title,
                            "journal": doc.journal,
                            "year": doc.year,
                            "chunk_type": "abstract_part",
                            "part_index": chunk_index
                        }
                    ))
                    chunk_index += 1
                
                current_chunk = sentence
        
        # æœ€åä¸€å—
        if current_chunk:
            chunks.append(TextChunk(
                text=f"æ‘˜è¦: {current_chunk}",
                doc_id=doc.pmid,
                chunk_id=f"{doc.pmid}_abstract_{chunk_index}",
                metadata={
                    "pmid": doc.pmid,
                    "title": doc.title,
                    "journal": doc.journal,
                    "year": doc.year,
                    "chunk_type": "abstract_part",
                    "part_index": chunk_index
                }
            ))
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        
        sentences = []
        current = ""
        
        for i, char in enumerate(text):
            current += char
            if char in '.!?' and i + 1 < len(text) and text[i + 1] in ' \n':
                sentences.append(current.strip())
                current = ""
        
        if current.strip():
            sentences.append(current.strip())
        
        return [s for s in sentences if len(s) > 10]

# ===== å‘é‡å­˜å‚¨ç³»ç»Ÿ =====

class VectorStore:
    """å‘é‡å­˜å‚¨å’Œæ£€ç´¢"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.encoder = SentenceTransformer(model_name)
        self.index = None
        self.chunks = []
    
    def build_index(self, chunks: List[TextChunk]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        
        print(f"ğŸ” æ„å»ºå‘é‡ç´¢å¼•ï¼Œæ¨¡å‹: {self.model_name}")
        
        self.chunks = chunks
        texts = [chunk.text for chunk in chunks]
        
        print(f"  ğŸ“Š ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬å—...")
        embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # æ ‡å‡†åŒ–ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} å—, ç»´åº¦: {dimension}")
    
    def search(self, query: str, top_k: int = 15) -> List[Dict]:
        """æœç´¢ç›¸å…³å—"""
        
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»º")
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encoder.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append({
                    "chunk": chunk,
                    "score": float(score),
                    "text": chunk.text,
                    "metadata": chunk.metadata
                })
        
        return results
    
    def save(self, file_path: str):
        """ä¿å­˜ç´¢å¼•"""
        
        save_data = {
            "chunks": self.chunks,
            "model_name": self.model_name
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(save_data, f)
        
        faiss.write_index(self.index, file_path + ".faiss")
        print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {file_path}")
    
    def load(self, file_path: str) -> bool:
        """åŠ è½½ç´¢å¼•"""
        
        try:
            with open(file_path, 'rb') as f:
                save_data = pickle.load(f)
            
            self.chunks = save_data["chunks"]
            self.model_name = save_data["model_name"]
            self.index = faiss.read_index(file_path + ".faiss")
            
            print(f"ğŸ“‚ ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} å—")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False

# ===== RAGæŸ¥è¯¢å¤„ç†å™¨ =====

class RAGProcessor:
    """RAGæŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.query_templates = {
            "disease_mechanism": "è¯¥åŸºå› ä¸å“ªäº›ç–¾ç—…ç›¸å…³ï¼Ÿç–¾ç—…çš„å‘ç—…æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆä¸´åºŠéœ€æ±‚ï¼Ÿ",
            "treatment_strategy": "æœ‰å“ªäº›æ²»ç–—æ–¹æ³•å’Œç­–ç•¥ï¼ŸåŒ…æ‹¬è¯ç‰©ã€ç–—æ³•ç­‰ï¼Ÿä¸´åºŠç ”ç©¶ç°çŠ¶å¦‚ä½•ï¼Ÿ",
            "target_analysis": "è¯¥åŸºå› çš„ä½œç”¨é€šè·¯æ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›æ½œåœ¨æ²»ç–—é¶ç‚¹ï¼Ÿç ”ç©¶è¿›å±•å¦‚ä½•ï¼Ÿ"
        }
    
    async def process_query(self, gene: str, query_type: str, top_k: int = 15) -> str:
        """å¤„ç†RAGæŸ¥è¯¢"""
        
        print(f"ğŸ¤– RAGæŸ¥è¯¢: {gene} - {query_type}")
        
        # æ„å»ºæŸ¥è¯¢
        base_query = self.query_templates.get(query_type, "")
        full_query = f"{gene} {base_query}"
        
        # æ£€ç´¢ç›¸å…³å—
        relevant_chunks = self.vector_store.search(full_query, top_k)
        
        if not relevant_chunks:
            return f"æœªæ‰¾åˆ°ä¸ {gene} ç›¸å…³çš„ {query_type} ä¿¡æ¯ã€‚"
        
        print(f"  ğŸ“Š æ£€ç´¢åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³å—")
        
        # æ„å»ºprompt
        prompt = self._build_prompt(gene, query_type, relevant_chunks)
        
        # LLMç”Ÿæˆ
        response = call_llm(prompt)
        return response
    
    def _build_prompt(self, gene: str, query_type: str, relevant_chunks: List[Dict]) -> str:
        """æ„å»ºRAG prompt - åŸºäºæ–‡çŒ®çš„å¼•ç”¨ç³»ç»Ÿ"""
        
        # æŒ‰PMIDåˆ†ç»„chunksï¼Œåˆ›å»ºæ–‡çŒ®æ˜ å°„
        pmid_to_literature = {}
        literature_counter = 1
        
        # ä¸ºæ¯ä¸ªå”¯ä¸€PMIDåˆ†é…æ–‡çŒ®ç¼–å·
        seen_pmids = set()
        for chunk_info in relevant_chunks:
            chunk = chunk_info["chunk"]
            pmid = chunk.metadata.get("pmid", "")
            if pmid and pmid not in seen_pmids:
                pmid_to_literature[pmid] = f"æ–‡çŒ®{literature_counter}"
                literature_counter += 1
                seen_pmids.add(pmid)
        
        # æ•´ç†ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æ–‡çŒ®ç¼–å·æ ‡è®°
        context_blocks = []
        references_text = "\n\n## å‚è€ƒæ–‡çŒ®\n"
        
        for i, chunk_info in enumerate(relevant_chunks):
            chunk = chunk_info["chunk"]
            score = chunk_info["score"]
            pmid = chunk.metadata.get("pmid", "")
            
            # è·å–æ–‡çŒ®ç¼–å·
            if pmid in pmid_to_literature:
                lit_ref = pmid_to_literature[pmid]
                context_blocks.append(f"[{i+1}] æ¥æº{lit_ref}: {chunk.text}")
            else:
                context_blocks.append(f"[{i+1}] {chunk.text}")
        
        # ç”Ÿæˆå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        for pmid, lit_ref in pmid_to_literature.items():
            # ä»chunksä¸­è·å–æ–‡çŒ®ä¿¡æ¯
            for chunk_info in relevant_chunks:
                chunk = chunk_info["chunk"]
                if chunk.metadata.get("pmid") == pmid:
                    title = chunk.metadata.get("title", "")
                    journal = chunk.metadata.get("journal", "")
                    year = chunk.metadata.get("year", "")
                    references_text += f"{lit_ref}: PMID:{pmid}, {title}, {journal}, {year}\n"
                    break
        
        context_text = "\n\n".join(context_blocks)
        
        # ===== åŸºå› æŸ¥è¯¢ç›¸å…³ Prompts =====
        if query_type == "disease_mechanism":
            prompt = f"""ä½ æ˜¯èµ„æ·±åŒ»å­¦ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯æ·±å…¥åˆ†æåŸºå›  {gene} çš„ç–¾ç—…æœºåˆ¶ã€‚

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨æ—¶ï¼Œè¯·ä½¿ç”¨å¯¹åº”çš„æ–‡çŒ®ç¼–å·ï¼ˆå¦‚æ–‡çŒ®1ã€æ–‡çŒ®2ç­‰ï¼‰ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·è¿›è¡Œè¯¦ç»†åˆ†æï¼Œåœ¨é€‚å½“ä½ç½®æ·»åŠ å¼•ç”¨ã€‚

{references_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### ç–¾ç—…æœºåˆ¶ä¸ä¸´åºŠéœ€æ±‚åˆ†æï¼ˆGene: {gene}ï¼‰

#### 1. ç–¾ç—…å…³è”è°±
- **å¼ºå…³è”ç–¾ç—…**ï¼ˆç›´æ¥è‡´ç—…åŸºå› ï¼‰ï¼š
  - ç–¾ç—…åç§° | é—ä¼ æ¨¡å¼ | æ‚£ç—…ç‡ | è¯æ®ç­‰çº§ [æ–‡çŒ®X]
- **ä¸­ç­‰å…³è”ç–¾ç—…**ï¼ˆæ˜“æ„ŸåŸºå› ï¼‰ï¼š
  - ç–¾ç—…åç§° | ORå€¼/RRå€¼ | äººç¾¤é¢‘ç‡ | è¯æ®æ¥æº [æ–‡çŒ®X]
- **å¼±å…³è”ç–¾ç—…**ï¼ˆå¯èƒ½ç›¸å…³ï¼‰ï¼š
  - ç–¾ç—…åç§° | ç ”ç©¶ç°çŠ¶ | äº‰è®®ç‚¹ [æ–‡çŒ®X]

#### 2. åˆ†å­ç—…ç†æœºåˆ¶
- **æ­£å¸¸ç”Ÿç†åŠŸèƒ½**ï¼š
  - è›‹ç™½åŠŸèƒ½åŸŸå’Œæ´»æ€§ä½ç‚¹
  - ä¿¡å·é€šè·¯å’Œè°ƒæ§ç½‘ç»œ
  - ç»„ç»‡è¡¨è¾¾è°±å’Œäºšç»†èƒå®šä½
- **è‡´ç—…æœºåˆ¶**ï¼š
  - åŠŸèƒ½ä¸§å¤±å‹å˜å¼‚ï¼ˆLoFï¼‰ï¼šå…·ä½“å½±å“
  - åŠŸèƒ½è·å¾—å‹å˜å¼‚ï¼ˆGoFï¼‰ï¼šæœºåˆ¶æè¿°
  - ä¸»å¯¼è´Ÿæ•ˆåº”ï¼ˆDominant negativeï¼‰ï¼šåˆ†å­åŸºç¡€
- **åŸºå› å‹-è¡¨å‹å…³è”**ï¼š
  - ç‰¹å®šå˜å¼‚ä¸ä¸´åºŠè¡¨ç°çš„å¯¹åº”å…³ç³» [æ–‡çŒ®X]

#### 3. ä¸´åºŠéœ€æ±‚è¯„ä¼°
- **å·²æœ‰æ²»ç–—æ‰‹æ®µ**ï¼š
  - è¯ç‰©æ²»ç–—ï¼šå…·ä½“è¯ç‰©å’Œç–—æ•ˆ
  - åŸºå› æ²»ç–—ï¼šè¿›å±•å’ŒæŒ‘æˆ˜
  - å…¶ä»–å¹²é¢„ï¼šæ•ˆæœè¯„ä»·
- **æœªæ»¡è¶³éœ€æ±‚**ï¼š
  - æ²»ç–—ç©ºç™½ï¼šå“ªäº›äºšå‹/é˜¶æ®µç¼ºä¹æœ‰æ•ˆæ²»ç–—
  - ç–—æ•ˆå±€é™ï¼šç°æœ‰æ²»ç–—çš„ä¸è¶³
  - å®‰å…¨æ€§é—®é¢˜ï¼šå‰¯ä½œç”¨å’Œé£é™©
- **æœºä¼šè¯†åˆ«**ï¼š
  - æ–°é¶ç‚¹ï¼šåŸºäºæœºåˆ¶çš„æ½œåœ¨å¹²é¢„ä½ç‚¹
  - æ–°ç­–ç•¥ï¼šåˆ›æ–°æ²»ç–—æ€è·¯
  - ä¼˜å…ˆçº§ï¼šæŒ‰å¯è¡Œæ€§å’Œå½±å“åŠ›æ’åº

#### 4. ç ”ç©¶è¶‹åŠ¿ä¸å±•æœ›
- è¿‘æœŸç ”ç©¶çƒ­ç‚¹ [æ–‡çŒ®X]
- æŠ€æœ¯çªç ´å’Œæ–°å‘ç°
- æœªæ¥ç ”ç©¶æ–¹å‘å»ºè®®

æ³¨æ„ï¼šåªåŸºäºæä¾›çš„æ–‡çŒ®ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ æœªæåŠçš„å†…å®¹ã€‚å¯¹äºä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æ ‡æ³¨"æ–‡çŒ®æœªæ˜ç¡®è¯´æ˜"ã€‚"""

        elif query_type == "treatment_strategy":
            prompt = f"""ä½ æ˜¯ä¸´åºŠåŒ»å­¦å’Œè¯ç‰©å¼€å‘ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯å…¨é¢åˆ†æåŸºå›  {gene} ç›¸å…³çš„æ²»ç–—ç­–ç•¥ã€‚

åˆ†æç»´åº¦ï¼š
1. ç³»ç»Ÿæ¢³ç†æ‰€æœ‰æ²»ç–—æ–¹æ³•ï¼ˆå·²ä¸Šå¸‚ã€ä¸´åºŠè¯•éªŒã€ä¸´åºŠå‰ï¼‰
2. è¯„ä¼°å„æ²»ç–—ç­–ç•¥çš„æ•ˆæœã€å®‰å…¨æ€§å’Œé€‚ç”¨äººç¾¤
3. æ¯”è¾ƒä¸åŒæ²»ç–—æ–¹æ¡ˆçš„ä¼˜åŠ£åŠ¿
4. è¯†åˆ«è”åˆæ²»ç–—æœºä¼šå’Œä¸ªä½“åŒ–æ²»ç–—ç­–ç•¥
5. é¢„æµ‹æœªæ¥æ²»ç–—å‘å±•æ–¹å‘

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œè¯·ä½¿ç”¨ [æ–‡çŒ®1]ã€[æ–‡çŒ®2] ç­‰æ ‡è®°ï¼ˆå¯¹åº”ä¸Šè¿°æ–‡çŒ®ç¼–å·ï¼‰ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### æ²»ç–—ç­–ç•¥ç»¼åˆåˆ†æï¼ˆGene: {gene}ï¼‰

#### 1. æ²»ç–—æ–¹æ³•å…¨æ™¯å›¾
- **å·²ä¸Šå¸‚æ²»ç–—**ï¼š
  - å°åˆ†å­è¯ç‰©ï¼šåç§° | ä½œç”¨æœºåˆ¶ | é€‚åº”ç—‡ | å…³é”®ä¸´åºŠæ•°æ® [æ–‡çŒ®X]
  - ç”Ÿç‰©åˆ¶å‰‚ï¼šç±»å‹ | é¶ç‚¹ | ç–—æ•ˆ | å®‰å…¨æ€§ [æ–‡çŒ®X]
  - åŸºå› /ç»†èƒæ²»ç–—ï¼šæŠ€æœ¯å¹³å° | ä¸´åºŠåº”ç”¨ | é•¿æœŸéšè®¿ [æ–‡çŒ®X]
- **ä¸´åºŠè¯•éªŒé˜¶æ®µ**ï¼š
  - IIIæœŸï¼šè¯ç‰© | å…¥ç»„æ ‡å‡† | ä¸»è¦ç»ˆç‚¹ | é¢„æœŸå®Œæˆæ—¶é—´ [æ–‡çŒ®X]
  - IIæœŸï¼šåˆ›æ–°ç‚¹ | åˆæ­¥ç–—æ•ˆ | å®‰å…¨æ€§ä¿¡å· [æ–‡çŒ®X]
  - IæœŸï¼šæ–°æœºåˆ¶ | å‰‚é‡æ¢ç´¢ | æ—©æœŸä¿¡å· [æ–‡çŒ®X]
- **ä¸´åºŠå‰ç ”ç©¶**ï¼š
  - æ–°é¶ç‚¹éªŒè¯ï¼šå®éªŒè¯æ® | è½¬åŒ–æ½œåŠ› [æ–‡çŒ®X]
  - æ–°æŠ€æœ¯åº”ç”¨ï¼šCRISPRã€ASOã€siRNAç­‰ [æ–‡çŒ®X]

#### 2. ç–—æ•ˆä¸å®‰å…¨æ€§è¯„ä¼°
- **ç–—æ•ˆå¯¹æ¯”åˆ†æ**ï¼š
  - å®¢è§‚ç¼“è§£ç‡ï¼ˆORRï¼‰ã€æ— è¿›å±•ç”Ÿå­˜æœŸï¼ˆPFSï¼‰ã€æ€»ç”Ÿå­˜æœŸï¼ˆOSï¼‰æ¯”è¾ƒ
  - ç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç–—æ•ˆé¢„æµ‹
  - çœŸå®ä¸–ç•Œæ•°æ® vs ä¸´åºŠè¯•éªŒæ•°æ®
- **å®‰å…¨æ€§æ¦‚å†µ**ï¼š
  - å¸¸è§ä¸è‰¯ååº”ï¼šå‘ç”Ÿç‡å’Œå¤„ç†ç­–ç•¥
  - ä¸¥é‡ä¸è‰¯äº‹ä»¶ï¼šé£é™©å› ç´ å’Œç›‘æµ‹æ–¹æ¡ˆ
  - ç‰¹æ®Šäººç¾¤è€ƒè™‘ï¼šå„¿ç«¥ã€è€å¹´ã€è‚è‚¾åŠŸèƒ½ä¸å…¨

#### 3. ä¸ªä½“åŒ–æ²»ç–—ç­–ç•¥
- **ç”Ÿç‰©æ ‡å¿—ç‰©æŒ‡å¯¼**ï¼š
  - é¢„æµ‹æ ‡å¿—ç‰©ï¼šåŸºå› å‹ã€è¡¨è¾¾æ°´å¹³ã€è›‹ç™½æ´»æ€§
  - ç›‘æµ‹æ ‡å¿—ç‰©ï¼šç–—æ•ˆè¯„ä¼°ã€è€è¯é¢„è­¦
- **è”åˆæ²»ç–—æ–¹æ¡ˆ**ï¼š
  - ç†è®ºåŸºç¡€ï¼šååŒæœºåˆ¶
  - ä¸´åºŠè¯æ®ï¼šè”åˆ vs å•è¯
  - æœ€ä½³ç»„åˆï¼šæ¨èæ–¹æ¡ˆ
- **åºè´¯æ²»ç–—ç­–ç•¥**ï¼š
  - ä¸€çº¿ã€äºŒçº¿ã€ä¸‰çº¿é€‰æ‹©é€»è¾‘
  - è€è¯åç­–ç•¥

#### 4. åˆ›æ–°æ²»ç–—å±•æœ›
- **çªç ´æ€§ç–—æ³•**ï¼šæŠ€æœ¯åˆ›æ–°ç‚¹å’Œä¸´åºŠè½¬åŒ–å‰æ™¯ [æ–‡çŒ®X]
- **ç²¾å‡†åŒ»ç–—å®è·µ**ï¼šä»åŸºå› æ£€æµ‹åˆ°æ²»ç–—å†³ç­–çš„è·¯å¾„
- **æœªæ¥5-10å¹´é¢„æµ‹**ï¼šå¯èƒ½æ”¹å˜æ²»ç–—æ ¼å±€çš„è¿›å±•

æ³¨æ„ï¼šè¯·ä¸¥æ ¼åŸºäºæ–‡çŒ®è¯æ®ï¼Œå¯¹äºæ¨æµ‹æ€§å†…å®¹éœ€æ˜ç¡®æ ‡æ³¨ã€‚"""

        elif query_type == "target_analysis":
            prompt = f"""ä½ æ˜¯è¯ç‰©é¶ç‚¹ç ”ç©¶å’Œæ–°è¯å¼€å‘ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯æ·±å…¥åˆ†æåŸºå›  {gene} çš„æˆè¯æ€§å’Œé¶ç‚¹å¼€å‘ç­–ç•¥ã€‚

åˆ†ææ¡†æ¶ï¼š
1. é¶ç‚¹å¯æˆè¯æ€§ï¼ˆDruggabilityï¼‰å¤šç»´åº¦è¯„ä¼°
2. ç»“æ„ç”Ÿç‰©å­¦åŸºç¡€å’Œè¯ç‰©è®¾è®¡ç­–ç•¥
3. é¶ç‚¹éªŒè¯è¯æ®é“¾å’Œè½¬åŒ–åŒ»å­¦è€ƒè™‘
4. çŸ¥è¯†äº§æƒæ ¼å±€å’Œç«äº‰æ€åŠ¿
5. å¼€å‘é£é™©è¯„ä¼°å’Œç¼“è§£ç­–ç•¥

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œè¯·ä½¿ç”¨ [æ–‡çŒ®1]ã€[æ–‡çŒ®2] ç­‰æ ‡è®°ï¼ˆå¯¹åº”ä¸Šè¿°æ–‡çŒ®ç¼–å·ï¼‰ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### é¶ç‚¹æˆè¯æ€§ä¸å¼€å‘ç­–ç•¥åˆ†æï¼ˆGene: {gene}ï¼‰

#### 1. é¶ç‚¹å¯æˆè¯æ€§è¯„ä¼°
- **è›‹ç™½ç»“æ„ç‰¹å¾**ï¼š
  - å·²è§£æç»“æ„ï¼šPDB ID | åˆ†è¾¨ç‡ | å…³é”®åŠŸèƒ½åŸŸ [æ–‡çŒ®X]
  - å¯ç»“åˆå£è¢‹ï¼šä½ç½® | å¤§å° | ç–æ°´æ€§ | å¯åŠæ€§è¯„åˆ†
  - å˜æ„ä½ç‚¹ï¼šå·²çŸ¥/é¢„æµ‹ä½ç‚¹åŠè°ƒæ§æœºåˆ¶
- **åŒ–å­¦å¯å¹²é¢„æ€§**ï¼š
  - å°åˆ†å­ç»“åˆï¼šå·²çŸ¥é…ä½“ | äº²å’ŒåŠ› | é€‰æ‹©æ€§ [æ–‡çŒ®X]
  - ç”Ÿç‰©å¤§åˆ†å­ï¼šæŠ—ä½“ã€å¤šè‚½ã€æ ¸é…¸é€‚é…ä½“å¯è¡Œæ€§
  - æ–°æ¨¡æ€ï¼šPROTACã€åˆ†å­èƒ¶ã€å…±ä»·æŠ‘åˆ¶å‰‚æ½œåŠ›
- **ç”Ÿç‰©å­¦å¯è¡Œæ€§**ï¼š
  - é¶ç‚¹é€‰æ‹©æ€§ï¼šé¿å…è„±é¶æ•ˆåº”çš„ç­–ç•¥
  - ç»„ç»‡åˆ†å¸ƒï¼šé¶å™¨å®˜å¯åŠæ€§
  - è¡¥å¿æœºåˆ¶ï¼šæ½œåœ¨è€è¯é€šè·¯

#### 2. é¶ç‚¹éªŒè¯å¼ºåº¦
- **é—ä¼ å­¦è¯æ®**ï¼š
  - äººç±»é—ä¼ å­¦ï¼šGWASã€ç½•è§å˜å¼‚ã€å®¶ç³»ç ”ç©¶ [æ–‡çŒ®X]
  - åŠŸèƒ½ç¼ºå¤±/è·å¾—å‹å˜å¼‚çš„è¡¨å‹
  - åŸºå› å‰‚é‡æ•ˆåº”
- **è¯ç†å­¦éªŒè¯**ï¼š
  - å·¥å…·åŒ–åˆç‰©ï¼šæ´»æ€§ã€é€‰æ‹©æ€§ã€ä½“å†…æ•ˆæœ [æ–‡çŒ®X]
  - åŸºå› æ•²é™¤/æ•²å‡ï¼šè¡¨å‹æ•‘æ´å®éªŒ
  - ç”Ÿç‰©æ ‡å¿—ç‰©ï¼šé¶ç‚¹å æœ‰ç‡ä¸æ•ˆæœå…³ç³»
- **ä¸´åºŠéªŒè¯**ï¼š
  - æ¦‚å¿µéªŒè¯ç ”ç©¶ï¼šæ—©æœŸä¸´åºŠä¿¡å· [æ–‡çŒ®X]
  - å¤±è´¥æ¡ˆä¾‹åˆ†æï¼šåŸå› å’Œå¯ç¤º

#### 3. è¯ç‰©è®¾è®¡ç­–ç•¥
- **åŸºäºç»“æ„çš„è®¾è®¡ï¼ˆSBDDï¼‰**ï¼š
  - å…ˆå¯¼åŒ–åˆç‰©ï¼šæ¥æºå’Œä¼˜åŒ–ç­–ç•¥
  - æ„æ•ˆå…³ç³»ï¼ˆSARï¼‰ï¼šå…³é”®è¯æ•ˆå›¢
  - è®¡ç®—è¾…åŠ©ï¼šåˆ†å­å¯¹æ¥ã€åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ
- **åŸºäºè¡¨å‹çš„ç­›é€‰**ï¼š
  - ç­›é€‰æ¨¡å‹ï¼šç»†èƒç³»ã€ç±»å™¨å®˜ã€åŠ¨ç‰©æ¨¡å‹
  - æ£€æµ‹æŒ‡æ ‡ï¼šä¸ä¸´åºŠç»ˆç‚¹çš„ç›¸å…³æ€§
- **æ–°æŠ€æœ¯åº”ç”¨**ï¼š
  - AI/MLåœ¨å…ˆå¯¼ç‰©å‘ç°ä¸­çš„åº”ç”¨
  - DNAç¼–ç åŒ–åˆç‰©åº“ï¼ˆDELï¼‰
  - Fragment-based drug discoveryï¼ˆFBDDï¼‰

#### 4. è½¬åŒ–åŒ»å­¦è€ƒè™‘
- **æ‚£è€…åˆ†å±‚ç­–ç•¥**ï¼š
  - ç”Ÿç‰©æ ‡å¿—ç‰©å¼€å‘ï¼šä¼´éšè¯Šæ–­
  - é€‚åº”ç—‡é€‰æ‹©ï¼šä¼˜å…ˆçº§æ’åº
- **ä¸´åºŠå¼€å‘è·¯å¾„**ï¼š
  - æ³¨å†Œè·¯å¾„ï¼šå­¤å„¿è¯ã€çªç ´æ€§ç–—æ³•è®¤å®šå¯èƒ½æ€§
  - ä¸´åºŠè¯•éªŒè®¾è®¡ï¼šç»ˆç‚¹é€‰æ‹©ã€æ ·æœ¬é‡ä¼°ç®—
- **å•†ä¸šåŒ–æ½œåŠ›**ï¼š
  - å¸‚åœºè§„æ¨¡ï¼šæ‚£è€…äººæ•°ã€æ²»ç–—æ¸—é€ç‡
  - ç«äº‰æ ¼å±€ï¼šåœ¨ç ”ç®¡çº¿åˆ†æ
  - å·®å¼‚åŒ–å®šä½ï¼šç‹¬ç‰¹ä»·å€¼ä¸»å¼ 

#### 5. é£é™©ä¸æœºé‡
- **æŠ€æœ¯é£é™©**ï¼šä¸»è¦æŒ‘æˆ˜å’Œåº”å¯¹æ–¹æ¡ˆ
- **ç›‘ç®¡é£é™©**ï¼šæ½œåœ¨çš„å®‰å…¨æ€§æ‹…å¿§
- **å•†ä¸šé£é™©**ï¼šIPå£å’ã€å¸‚åœºæ¥å—åº¦
- **æœºé‡çª—å£**ï¼šæ—¶é—´æ•æ„Ÿæ€§åˆ†æ

æ³¨æ„ï¼šè¯„ä¼°åº”åŸºäºæ–‡çŒ®è¯æ®ï¼Œå¯¹æ¨æµ‹æ€§åˆ¤æ–­éœ€æ³¨æ˜ä¾æ®ã€‚"""


        elif query_type == "mechanism_pathway":
            prompt = f"""ä½ æ˜¯åˆ†å­ç”Ÿç‰©å­¦å’Œç³»ç»Ÿç”Ÿç‰©å­¦ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®æ·±å…¥è§£æä¸ "{gene}" ç›¸å…³çš„åˆ†å­æœºåˆ¶å’Œä¿¡å·é€šè·¯ã€‚

åˆ†æé‡ç‚¹ï¼š
1. æ ¸å¿ƒä¿¡å·é€šè·¯çš„è¯¦ç»†è§£æ
2. è°ƒæ§ç½‘ç»œå’Œåé¦ˆæœºåˆ¶
3. æ—¶ç©ºåŠ¨æ€å’Œç»†èƒç‰¹å¼‚æ€§
4. ä¸ç–¾ç—…çš„æœºåˆ¶è”ç³»
5. æ½œåœ¨çš„å¹²é¢„èŠ‚ç‚¹

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### åˆ†å­æœºåˆ¶ä¸é€šè·¯è§£æï¼š{gene}

#### 1. æ ¸å¿ƒä¿¡å·é€šè·¯
- **ç»å…¸é€šè·¯**ï¼š
  - é€šè·¯åç§°ï¼šå…³é”®åˆ†å­ â†’ ä¸­é—´æ­¥éª¤ â†’ ä¸‹æ¸¸æ•ˆåº” [æ–‡çŒ®X]
  - è°ƒæ§æœºåˆ¶ï¼šæ¿€æ´»/æŠ‘åˆ¶æ¡ä»¶ã€åé¦ˆç¯è·¯
  - ç”Ÿç†åŠŸèƒ½ï¼šæ­£å¸¸çŠ¶æ€ä¸‹çš„ä½œç”¨
- **æ–°å‘ç°é€šè·¯**ï¼š
  - éç»å…¸æ¿€æ´»ï¼šæ–°çš„ä¸Šæ¸¸ä¿¡å·æˆ–æ¿€æ´»æ¨¡å¼ [æ–‡çŒ®X]
  - äº¤å‰å¯¹è¯ï¼šä¸å…¶ä»–é€šè·¯çš„ç›¸äº’ä½œç”¨
  - ç»†èƒç±»å‹ç‰¹å¼‚æ€§ï¼šä¸åŒç»†èƒä¸­çš„å·®å¼‚

#### 2. åˆ†å­ç›¸äº’ä½œç”¨ç½‘ç»œ
- **ç›´æ¥ç›¸äº’ä½œç”¨**ï¼š
  - è›‹ç™½-è›‹ç™½ï¼šç»“åˆç•Œé¢ã€äº²å’ŒåŠ›ã€åŠŸèƒ½å½±å“ [æ–‡çŒ®X]
  - è›‹ç™½-æ ¸é…¸ï¼šç»“åˆåºåˆ—ã€è°ƒæ§æ¨¡å¼
  - ç¿»è¯‘åä¿®é¥°ï¼šç±»å‹ã€ä½ç‚¹ã€åŠŸèƒ½åæœ
- **é—´æ¥è°ƒæ§**ï¼š
  - è½¬å½•è°ƒæ§ï¼šè½¬å½•å› å­ã€å¢å¼ºå­ã€è¡¨è§‚é—ä¼ 
  - ä»£è°¢è°ƒæ§ï¼šä»£è°¢ç‰©åé¦ˆã€èƒ½é‡æ„Ÿåº”
  - å¾®ç¯å¢ƒå› ç´ ï¼špHã€æ°§æµ“åº¦ã€æœºæ¢°åŠ›

#### 3. æ—¶ç©ºåŠ¨æ€è°ƒæ§
- **æ—¶é—´åŠ¨æ€**ï¼š
  - å¿«é€Ÿå“åº”ï¼ˆåˆ†é’Ÿçº§ï¼‰ï¼šç£·é…¸åŒ–çº§è”
  - ä¸­æœŸé€‚åº”ï¼ˆå°æ—¶çº§ï¼‰ï¼šè½¬å½•é‡ç¼–ç¨‹
  - é•¿æœŸé‡å¡‘ï¼ˆå¤©-å‘¨ï¼‰ï¼šè¡¨è§‚é—ä¼ æ”¹å˜
- **ç©ºé—´åˆ†å¸ƒ**ï¼š
  - äºšç»†èƒå®šä½ï¼šå®šä½ä¿¡å·ã€è½¬è¿æœºåˆ¶
  - ç»„ç»‡ç‰¹å¼‚æ€§ï¼šè¡¨è¾¾è°±ã€åŠŸèƒ½å·®å¼‚
  - å‘è‚²é˜¶æ®µæ€§ï¼šæ—¶åºè¡¨è¾¾ã€åŠŸèƒ½è½¬æ¢

#### 4. ç—…ç†çŠ¶æ€æ”¹å˜
- **å¤±è°ƒæœºåˆ¶**ï¼šæ­£å¸¸â†’ç–¾ç—…çš„åˆ†å­äº‹ä»¶é“¾ [æ–‡çŒ®X]
- **ä»£å¿æœºåˆ¶**ï¼šæœºä½“çš„é€‚åº”æ€§æ”¹å˜
- **æ²»ç–—é¶ç‚¹**ï¼šå¯å¹²é¢„çš„å…³é”®èŠ‚ç‚¹

æ³¨æ„ï¼šä¼˜å…ˆå¼•ç”¨æœ€æ–°ç ”ç©¶ï¼Œæ³¨æ„æœºåˆ¶çš„äº‰è®®å’Œä¸ç¡®å®šæ€§ã€‚"""

        else:
            # é€šç”¨promptå¤„ç†å…¶ä»–query_type
            prompt = f"""è¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯åˆ†æåŸºå›  {gene} çš„ {query_type}ã€‚

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨æ—¶ï¼Œè¯·ä½¿ç”¨å¯¹åº”çš„æ–‡çŒ®ç¼–å·ï¼ˆå¦‚æ–‡çŒ®1ã€æ–‡çŒ®2ç­‰ï¼‰ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·è¿›è¡Œè¯¦ç»†åˆ†æï¼Œåœ¨é€‚å½“ä½ç½®æ·»åŠ å¼•ç”¨ã€‚

{references_text}"""

        return prompt
# ===== ç¼“å­˜ç®¡ç†å™¨ =====

class CacheManager:
    """å¢å¼ºçš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "enhanced_literature_cache"):
        self.cache_dir = cache_dir
        self.cache_days = 7  # ç¼“å­˜æœ‰æ•ˆæœŸ
        os.makedirs(cache_dir, exist_ok=True)
    
    def load_by_key(self, cache_key: str) -> Optional[VectorStore]:
        """æ ¹æ®ç¼“å­˜é”®åŠ è½½"""
        
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            # æ£€æŸ¥ç¼“å­˜æ—¶æ•ˆ
            mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
            if datetime.now() - mod_time > timedelta(days=self.cache_days):
                return None
            
            with open(cache_file, 'rb') as f:
                vector_store = pickle.load(f)
                print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½: {cache_key}")
                return vector_store
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def save_by_key(self, cache_key: str, vector_store: VectorStore):
        """æ ¹æ®ç¼“å­˜é”®ä¿å­˜"""
        
        try:
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            with open(cache_file, 'wb') as f:
                pickle.dump(vector_store, f)
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_key}")
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # å…¼å®¹åŸæœ‰æ¥å£
    def get_cache_path(self, gene: str, max_results: int) -> str:
        """è·å–ç¼“å­˜è·¯å¾„"""
        cache_key = f"{gene}_{max_results}"
        return os.path.join(self.cache_dir, f"{cache_key}")
    
    def is_valid(self, cache_path: str, max_age_days: int = 7) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        return datetime.now() - file_time < timedelta(days=max_age_days)
    
    def save(self, gene: str, max_results: int, vector_store: VectorStore):
        """å…¼å®¹åŸæœ‰saveæ–¹æ³•"""
        cache_key = f"{gene}_{max_results}"
        self.save_by_key(cache_key, vector_store)
    
    def load(self, gene: str, max_results: int) -> Optional[VectorStore]:
        """å…¼å®¹åŸæœ‰loadæ–¹æ³•"""
        cache_key = f"{gene}_{max_results}"
        return self.load_by_key(cache_key)

# ===== ä¸»è¦çš„Literature Expert =====

class LiteratureExpert:
    """æ–‡çŒ®åˆ†æä¸“å®¶ - åŸºäºRAGä¼˜åŒ–ï¼Œæ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹"""
    
    def __init__(self, config: AnalysisConfig = None):
        self.name = "Enhanced Literature Expert"
        self.version = "3.0.0"
        self.expertise = ["å¤šç±»å‹æŸ¥è¯¢", "æ–‡çŒ®åˆ†æ", "æœºåˆ¶ç ”ç©¶", "æ²»ç–—ç­–ç•¥", "é¶ç‚¹åˆ†æ"]
        
        # é…ç½®
        self.config = config or ConfigManager.get_standard_config()
        
        # ç»„ä»¶
        self.retriever = PubMedRetriever()
        self.chunker = SmartChunker(chunk_size=250, overlap=50)
        self.cache_manager = CacheManager()
        
        logger.info(f"Literature Expert åˆå§‹åŒ–å®Œæˆ - {self.version}")
    
    def set_config(self, config: AnalysisConfig):
        """è®¾ç½®é…ç½®"""
        self.config = config
        logger.info(f"é…ç½®å·²æ›´æ–°: {config.mode.value}")
    
    def set_mode(self, mode: AnalysisMode):
        """è®¾ç½®æ¨¡å¼"""
        self.config = ConfigManager.get_config_by_mode(mode)
        logger.info(f"æ¨¡å¼åˆ‡æ¢: {mode.value}")
    
    async def analyze(self, gene_target: str, context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """
        ä¸»è¦åˆ†ææ–¹æ³•ï¼ˆåŸºå› åæŸ¥è¯¢ï¼‰
        
        Args:
            gene_target: ç›®æ ‡åŸºå› 
            context: ä¸Šä¸‹æ–‡é…ç½®
        
        Returns:
            æ–‡çŒ®åˆ†æç»“æœ
        """
        
        return await self.analyze_by_gene(gene_target, context)
    
    async def analyze_by_gene(self, gene_target: str, context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """åŸºå› ååˆ†æ"""
        
        query = SearchQuery(
            query_text=gene_target,
            query_type=QueryType.GENE,
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_by_keyword(self, keyword: str, 
                                additional_terms: List[str] = None,
                                exclude_terms: List[str] = None,
                                context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """å…³é”®è¯åˆ†æ"""
        
        query = SearchQuery(
            query_text=keyword,
            query_type=QueryType.KEYWORD,
            additional_terms=additional_terms or [],
            exclude_terms=exclude_terms or [],
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_protein_family(self, family_name: str,
                                   additional_terms: List[str] = None,
                                   context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """è›‹ç™½å®¶æ—åˆ†æ"""
        
        query = SearchQuery(
            query_text=family_name,
            query_type=QueryType.PROTEIN_FAMILY,
            additional_terms=additional_terms or [],
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_mechanism(self, mechanism_query: str,
                              additional_terms: List[str] = None,
                              context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """æœºåˆ¶åˆ†æ"""
        
        query = SearchQuery(
            query_text=mechanism_query,
            query_type=QueryType.MECHANISM,
            additional_terms=additional_terms or [],
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_by_query(self, search_query: SearchQuery, context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """
        é€šç”¨æŸ¥è¯¢åˆ†ææ–¹æ³•
        
        Args:
            search_query: æœç´¢æŸ¥è¯¢å¯¹è±¡
            context: ä¸Šä¸‹æ–‡é…ç½®
        
        Returns:
            æ–‡çŒ®åˆ†æç»“æœ
        """
        
        logger.info(f"å¼€å§‹æ–‡çŒ®åˆ†æ: {search_query.query_text} ({search_query.query_type.value}) - æ¨¡å¼: {self.config.mode.value}")
        
        try:
            # ç¡®å®šåˆ†æå‚æ•°
            top_k = self._get_top_k()
            
            # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
            cache_key = self._generate_cache_key(search_query)
            vector_store = self.cache_manager.load_by_key(cache_key)
            
            # 2. å¦‚æœç¼“å­˜æ— æ•ˆï¼Œé‡æ–°æ„å»º
            if vector_store is None:
                vector_store = await self._build_literature_index_async(search_query)
                # ä¿å­˜ç¼“å­˜
                self.cache_manager.save_by_key(cache_key, vector_store)
            
            # 3. RAGæŸ¥è¯¢å¤„ç†
            rag_processor = RAGProcessor(vector_store)
            
            print("ğŸ¤– å¼€å§‹RAGæŸ¥è¯¢...")
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´RAGæŸ¥è¯¢
            rag_queries = self._get_rag_queries(search_query)
            
            # å¹¶å‘å¤„ç†æŸ¥è¯¢
            tasks = [
                rag_processor.process_query(search_query.query_text, query_type, top_k)
                for query_type in rag_queries
            ]
            
            results = await asyncio.gather(*tasks)
            
            # 4. æ„å»ºåˆ†æç»“æœ
            references = self._extract_references(vector_store.chunks)
            confidence_score = self._calculate_confidence(vector_store.chunks)
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹ç»„ç»‡ç»“æœ
            result_dict = {}
            for i, query_type in enumerate(rag_queries):
                result_dict[query_type] = results[i] if i < len(results) else ""
            
            analysis_result = LiteratureAnalysisResult(
                gene_target=search_query.query_text,  # ä¿æŒå…¼å®¹æ€§
                disease_mechanism=result_dict.get("disease_mechanism", ""),
                treatment_strategy=result_dict.get("treatment_strategy", ""),
                target_analysis=result_dict.get("target_analysis", ""),
                references=references[:50],  # é™åˆ¶å¼•ç”¨æ•°é‡
                total_literature=len(set(chunk.doc_id for chunk in vector_store.chunks)),
                total_chunks=len(vector_store.chunks),
                confidence_score=confidence_score,
                analysis_method=f"Enhanced-RAG-{search_query.query_type.value}",
                timestamp=datetime.now().isoformat(),
                config_used=self._get_config_summary(),
                token_usage=self._estimate_token_usage(top_k)
            )
            
            logger.info(f"æ–‡çŒ®åˆ†æå®Œæˆ: {search_query.query_text} - æ–‡çŒ®æ•°: {analysis_result.total_literature}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®åˆ†æå¤±è´¥: {search_query.query_text} - {str(e)}")
            return self._create_error_result(search_query.query_text, str(e))
    
    def _get_rag_queries(self, search_query: SearchQuery) -> List[str]:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è·å–RAGæŸ¥è¯¢ç±»å‹"""
        
        # æ‰€æœ‰æŸ¥è¯¢ç±»å‹éƒ½ä½¿ç”¨æ ‡å‡†çš„ä¸‰ä¸ªåˆ†æç»´åº¦
        return ["disease_mechanism", "treatment_strategy", "target_analysis"]
    
    def _generate_cache_key(self, search_query: SearchQuery) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        
        query_str = f"{search_query.query_text}_{search_query.query_type.value}"
        if search_query.additional_terms:
            query_str += f"_add_{','.join(search_query.additional_terms)}"
        if search_query.exclude_terms:
            query_str += f"_exc_{','.join(search_query.exclude_terms)}"
        if search_query.date_range:
            query_str += f"_date_{search_query.date_range[0]}_{search_query.date_range[1]}"
        
        return hashlib.md5(query_str.encode()).hexdigest()
    
    async def _build_literature_index_async(self, search_query: SearchQuery) -> VectorStore:
        """æ„å»ºæ–‡çŒ®ç´¢å¼•"""
        
        print(f"ğŸ—ï¸ æ„å»ºæ–‡çŒ®ç´¢å¼•: {search_query.query_text} ({search_query.query_type.value})")
        
        # 1. æ£€ç´¢æ–‡çŒ®
        documents = await self.retriever.search_literature(search_query)
        
        if not documents:
            raise ValueError(f"æœªæ‰¾åˆ° {search_query.query_text} ç›¸å…³æ–‡çŒ®")
        
        # 2. æ–‡æœ¬åˆ†å—
        chunks = self.chunker.chunk_documents(documents)
        
        # 3. æ„å»ºå‘é‡ç´¢å¼•
        vector_store = VectorStore()
        vector_store.build_index(chunks)
        
        return vector_store
    
    def _get_max_literature(self) -> int:
        """è·å–æœ€å¤§æ–‡çŒ®æ•°é‡"""
        if self.config.mode == AnalysisMode.QUICK:
            return 100
        elif self.config.mode == AnalysisMode.STANDARD:
            return 500
        elif self.config.mode == AnalysisMode.DEEP:
            # return 1000
            return 10
        else:
            return 500
    
    def _get_top_k(self) -> int:
        """è·å–top-kå‚æ•°"""
        if self.config.mode == AnalysisMode.QUICK:
            return 25
        elif self.config.mode == AnalysisMode.STANDARD:
            return 100
        elif self.config.mode == AnalysisMode.DEEP:
            # return 300
            return 10
        else:
            return 50
    
    def _extract_references(self, chunks: List[TextChunk]) -> List[Dict]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        
        references = {}
        for chunk in chunks:
            pmid = chunk.metadata.get("pmid", "")
            if pmid and pmid not in references:
                references[pmid] = {
                    "PMID": pmid,
                    "Title": chunk.metadata.get("title", ""),
                    "Journal": chunk.metadata.get("journal", ""),
                    "Year": chunk.metadata.get("year", 0)
                }
        
        return list(references.values())
    
    def _calculate_confidence(self, chunks: List[TextChunk]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        
        if not chunks:
            return 0.0
        
        # åŸºäºæ–‡çŒ®æ•°é‡å’Œè´¨é‡çš„ç®€å•è¯„åˆ†
        unique_docs = len(set(chunk.doc_id for chunk in chunks))
        
        if unique_docs >= 50:
            return 0.9
        elif unique_docs >= 20:
            return 0.8
        elif unique_docs >= 10:
            return 0.7
        elif unique_docs >= 5:
            return 0.6
        else:
            return 0.5
    
    def _estimate_token_usage(self, top_k: int) -> Dict:
        """ä¼°ç®—Tokenä½¿ç”¨"""
        
        # RAGæ–¹å¼çš„Tokenä¼°ç®—
        input_tokens = top_k * 200  # æ¯ä¸ªç›¸å…³å—çº¦200 tokens
        output_tokens = 10000 * 3   # ä¸‰ä¸ªé—®é¢˜å„1000 tokensè¾“å‡º
        total_tokens = input_tokens + output_tokens
        
        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "estimated_cost_usd": total_tokens * 0.000002
        }
    
    def _get_config_summary(self) -> Dict:
        """è·å–é…ç½®æ‘˜è¦"""
        
        return {
            "mode": self.config.mode.value,
            "max_literature": self._get_max_literature(),
            "top_k": self._get_top_k(),
            "analysis_method": "RAG-optimized"
        }
    
    def _create_error_result(self, gene_target: str, error_msg: str) -> LiteratureAnalysisResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        
        return LiteratureAnalysisResult(
            gene_target=gene_target,
            disease_mechanism=f"åˆ†æ {gene_target} æ—¶å‘ç”Ÿé”™è¯¯: {error_msg}",
            treatment_strategy="",
            target_analysis="",
            references=[],
            total_literature=0,
            total_chunks=0,
            confidence_score=0.0,
            analysis_method="error",
            timestamp=datetime.now().isoformat(),
            config_used=self._get_config_summary(),
            token_usage={}
        )
    
    def export_results(self, result: LiteratureAnalysisResult, format: str = "dict") -> Any:
        """å¯¼å‡ºç»“æœ"""
        
        if format == "dict":
            return asdict(result)
        elif format == "json":
            import json
            return json.dumps(asdict(result), indent=2, ensure_ascii=False)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
    
    def estimate_analysis_cost(self, gene_target: str) -> Dict[str, Any]:
        """ä¼°ç®—åˆ†ææˆæœ¬"""
        
        token_estimate = self._estimate_token_usage(self._get_top_k())
        
        return {
            "gene_target": gene_target,
            "estimated_tokens": token_estimate["total_tokens"],
            "estimated_cost_usd": token_estimate["estimated_cost_usd"],
            "estimated_time_seconds": 60,  # RAGåˆ†æçº¦1åˆ†é’Ÿ
            "config_mode": self.config.mode.value,
            "max_literature": self._get_max_literature()
        }
    
    def __str__(self) -> str:
        return f"LiteratureExpert(name='{self.name}', version='{self.version}', mode='{self.config.mode.value}')"

